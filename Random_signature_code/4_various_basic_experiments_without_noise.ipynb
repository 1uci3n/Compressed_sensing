{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import data_tool as dt\n",
    "import keras_tool as kt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_number = 100\n",
    "code_length = 50\n",
    "epoch_number = 30\n",
    "batch_size = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bp1 (BatchNormalization)     (3, 50)                   200       \n",
      "_________________________________________________________________\n",
      "layer1 (Dense)               (3, 150)                  7650      \n",
      "_________________________________________________________________\n",
      "bp2 (BatchNormalization)     (3, 150)                  600       \n",
      "_________________________________________________________________\n",
      "layer2 (Dense)               (3, 150)                  22650     \n",
      "_________________________________________________________________\n",
      "bp3 (BatchNormalization)     (3, 150)                  600       \n",
      "_________________________________________________________________\n",
      "layer3 (Dense)               (3, 150)                  22650     \n",
      "_________________________________________________________________\n",
      "pb4 (BatchNormalization)     (3, 150)                  600       \n",
      "_________________________________________________________________\n",
      "layer4 (Dense)               (3, 100)                  15100     \n",
      "_________________________________________________________________\n",
      "bp1_2 (BatchNormalization)   (3, 100)                  400       \n",
      "_________________________________________________________________\n",
      "layer1_2 (Dense)             (3, 150)                  15150     \n",
      "_________________________________________________________________\n",
      "bp2_2 (BatchNormalization)   (3, 150)                  600       \n",
      "_________________________________________________________________\n",
      "layer2_2 (Dense)             (3, 150)                  22650     \n",
      "_________________________________________________________________\n",
      "bp3_2 (BatchNormalization)   (3, 150)                  600       \n",
      "_________________________________________________________________\n",
      "layer3_2 (Dense)             (3, 150)                  22650     \n",
      "_________________________________________________________________\n",
      "pb4_2 (BatchNormalization)   (3, 150)                  600       \n",
      "_________________________________________________________________\n",
      "layer4_2 (Dense)             (3, 100)                  15100     \n",
      "_________________________________________________________________\n",
      "bp1_3 (BatchNormalization)   (3, 100)                  400       \n",
      "_________________________________________________________________\n",
      "layer1_3 (Dense)             (3, 150)                  15150     \n",
      "_________________________________________________________________\n",
      "bp2_3 (BatchNormalization)   (3, 150)                  600       \n",
      "_________________________________________________________________\n",
      "layer2_3 (Dense)             (3, 150)                  22650     \n",
      "_________________________________________________________________\n",
      "bp3_3 (BatchNormalization)   (3, 150)                  600       \n",
      "_________________________________________________________________\n",
      "layer3_3 (Dense)             (3, 150)                  22650     \n",
      "_________________________________________________________________\n",
      "pb4_3 (BatchNormalization)   (3, 150)                  600       \n",
      "_________________________________________________________________\n",
      "layer4_3 (Dense)             (3, 100)                  15100     \n",
      "_________________________________________________________________\n",
      "bp1_4 (BatchNormalization)   (3, 100)                  400       \n",
      "_________________________________________________________________\n",
      "layer1_4 (Dense)             (3, 150)                  15150     \n",
      "_________________________________________________________________\n",
      "bp2_4 (BatchNormalization)   (3, 150)                  600       \n",
      "_________________________________________________________________\n",
      "layer2_4 (Dense)             (3, 150)                  22650     \n",
      "_________________________________________________________________\n",
      "bp3_4 (BatchNormalization)   (3, 150)                  600       \n",
      "_________________________________________________________________\n",
      "layer3_4 (Dense)             (3, 150)                  22650     \n",
      "_________________________________________________________________\n",
      "pb4_4 (BatchNormalization)   (3, 150)                  600       \n",
      "_________________________________________________________________\n",
      "layer4_4 (Dense)             (3, 100)                  15100     \n",
      "_________________________________________________________________\n",
      "bp1_5 (BatchNormalization)   (3, 100)                  400       \n",
      "_________________________________________________________________\n",
      "layer1_5 (Dense)             (3, 150)                  15150     \n",
      "_________________________________________________________________\n",
      "bp2_5 (BatchNormalization)   (3, 150)                  600       \n",
      "_________________________________________________________________\n",
      "layer2_5 (Dense)             (3, 150)                  22650     \n",
      "_________________________________________________________________\n",
      "bp3_5 (BatchNormalization)   (3, 150)                  600       \n",
      "_________________________________________________________________\n",
      "layer3_5 (Dense)             (3, 150)                  22650     \n",
      "_________________________________________________________________\n",
      "pb4_5 (BatchNormalization)   (3, 150)                  600       \n",
      "_________________________________________________________________\n",
      "layer4_5 (Dense)             (3, 100)                  15100     \n",
      "=================================================================\n",
      "Total params: 381,050\n",
      "Trainable params: 375,650\n",
      "Non-trainable params: 5,400\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "decoder = keras.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.BatchNormalization(name=\"bp1\"),\n",
    "        layers.Dense(user_number + code_length, activation=\"relu\", name=\"layer1\"),\n",
    "        tf.keras.layers.BatchNormalization(name=\"bp2\"),\n",
    "        layers.Dense(user_number + code_length, activation=\"relu\", name=\"layer2\"),\n",
    "        tf.keras.layers.BatchNormalization(name=\"bp3\"),\n",
    "        layers.Dense(user_number + code_length, activation=\"relu\", name=\"layer3\"),\n",
    "        tf.keras.layers.BatchNormalization(name=\"pb4\"),\n",
    "        layers.Dense(user_number, activation=\"relu\", name=\"layer4\"),\n",
    "        tf.keras.layers.BatchNormalization(name=\"bp1_2\"),\n",
    "        layers.Dense(user_number + code_length, activation=\"relu\", name=\"layer1_2\"),\n",
    "        tf.keras.layers.BatchNormalization(name=\"bp2_2\"),\n",
    "        layers.Dense(user_number + code_length, activation=\"relu\", name=\"layer2_2\"),\n",
    "        tf.keras.layers.BatchNormalization(name=\"bp3_2\"),\n",
    "        layers.Dense(user_number + code_length, activation=\"relu\", name=\"layer3_2\"),\n",
    "        tf.keras.layers.BatchNormalization(name=\"pb4_2\"),\n",
    "        layers.Dense(user_number, activation=\"relu\", name=\"layer4_2\"),\n",
    "        tf.keras.layers.BatchNormalization(name=\"bp1_3\"),\n",
    "        layers.Dense(user_number + code_length, activation=\"relu\", name=\"layer1_3\"),\n",
    "        tf.keras.layers.BatchNormalization(name=\"bp2_3\"),\n",
    "        layers.Dense(user_number + code_length, activation=\"relu\", name=\"layer2_3\"),\n",
    "        tf.keras.layers.BatchNormalization(name=\"bp3_3\"),\n",
    "        layers.Dense(user_number + code_length, activation=\"relu\", name=\"layer3_3\"),\n",
    "        tf.keras.layers.BatchNormalization(name=\"pb4_3\"),\n",
    "        layers.Dense(user_number, activation=\"relu\", name=\"layer4_3\"),\n",
    "        tf.keras.layers.BatchNormalization(name=\"bp1_4\"),\n",
    "        layers.Dense(user_number + code_length, activation=\"relu\", name=\"layer1_4\"),\n",
    "        tf.keras.layers.BatchNormalization(name=\"bp2_4\"),\n",
    "        layers.Dense(user_number + code_length, activation=\"relu\", name=\"layer2_4\"),\n",
    "        tf.keras.layers.BatchNormalization(name=\"bp3_4\"),\n",
    "        layers.Dense(user_number + code_length, activation=\"relu\", name=\"layer3_4\"),\n",
    "        tf.keras.layers.BatchNormalization(name=\"pb4_4\"),\n",
    "        layers.Dense(user_number, activation=\"relu\", name=\"layer4_4\"),\n",
    "        tf.keras.layers.BatchNormalization(name=\"bp1_5\"),\n",
    "        layers.Dense(user_number + code_length, activation=\"relu\", name=\"layer1_5\"),\n",
    "        tf.keras.layers.BatchNormalization(name=\"bp2_5\"),\n",
    "        layers.Dense(user_number + code_length, activation=\"relu\", name=\"layer2_5\"),\n",
    "        tf.keras.layers.BatchNormalization(name=\"bp3_5\"),\n",
    "        layers.Dense(user_number + code_length, activation=\"relu\", name=\"layer3_5\"),\n",
    "        tf.keras.layers.BatchNormalization(name=\"pb4_5\"),\n",
    "        layers.Dense(user_number, activation=\"relu\", name=\"layer4_5\"),\n",
    "    ],name = 'decoder'\n",
    ")\n",
    "# Call model on a test input\n",
    "y = tf.ones((3, code_length))\n",
    "x_hat = decoder(y)\n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "signature_matrix = dt.get_random_binary_signature_matrix(user_number, code_length)\n",
    "y_set, x_set, h_set = dt.get_dataset(2500000, signature_matrix, is_fading=1)\n",
    "y_set = y_set.astype(\"float32\")\n",
    "x_set = x_set.astype(\"float32\")\n",
    "h_set = h_set.astype(\"float32\")\n",
    "y_test, x_test, h_test = dt.get_dataset(10000, signature_matrix, is_fading=1)\n",
    "y_test = y_test.astype(\"float32\")\n",
    "x_test = x_test.astype(\"float32\")\n",
    "h_test = h_test.astype(\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "2000/2000 [==============================] - 142s 68ms/step - loss: 0.1707 - nmse_accuracy: -0.8079 - val_loss: 0.1331 - val_nmse_accuracy: -1.9074\n",
      "Epoch 2/30\n",
      "2000/2000 [==============================] - 128s 64ms/step - loss: 0.1197 - nmse_accuracy: -2.3785 - val_loss: 0.1146 - val_nmse_accuracy: -2.6049\n",
      "Epoch 3/30\n",
      "2000/2000 [==============================] - 122s 61ms/step - loss: 0.1082 - nmse_accuracy: -2.8546 - val_loss: 0.1039 - val_nmse_accuracy: -3.0783\n",
      "Epoch 4/30\n",
      "2000/2000 [==============================] - 116s 58ms/step - loss: 0.0989 - nmse_accuracy: -3.2789 - val_loss: 0.0971 - val_nmse_accuracy: -3.3998\n",
      "Epoch 5/30\n",
      "2000/2000 [==============================] - 126s 63ms/step - loss: 0.0931 - nmse_accuracy: -3.5799 - val_loss: 0.0925 - val_nmse_accuracy: -3.5826\n",
      "Epoch 6/30\n",
      "2000/2000 [==============================] - 131s 65ms/step - loss: 0.0862 - nmse_accuracy: -3.9070 - val_loss: 0.0865 - val_nmse_accuracy: -3.9312\n",
      "Epoch 7/30\n",
      "2000/2000 [==============================] - 183s 92ms/step - loss: 0.0796 - nmse_accuracy: -4.3126 - val_loss: 0.0782 - val_nmse_accuracy: -4.4189\n",
      "Epoch 8/30\n",
      "2000/2000 [==============================] - 203s 101ms/step - loss: 0.0724 - nmse_accuracy: -4.7653 - val_loss: 0.0724 - val_nmse_accuracy: -4.7674\n",
      "Epoch 9/30\n",
      "2000/2000 [==============================] - 155s 78ms/step - loss: 0.0663 - nmse_accuracy: -5.1780 - val_loss: 0.0717 - val_nmse_accuracy: -4.7487\n",
      "Epoch 10/30\n",
      "2000/2000 [==============================] - 246s 123ms/step - loss: 0.0621 - nmse_accuracy: -5.4714 - val_loss: 0.0674 - val_nmse_accuracy: -5.0802\n",
      "Epoch 11/30\n",
      "2000/2000 [==============================] - 201s 101ms/step - loss: 0.0589 - nmse_accuracy: -5.7361 - val_loss: 0.0639 - val_nmse_accuracy: -5.3262\n",
      "Epoch 12/30\n",
      "2000/2000 [==============================] - 180s 90ms/step - loss: 0.0567 - nmse_accuracy: -5.8970 - val_loss: 0.0622 - val_nmse_accuracy: -5.4275\n",
      "Epoch 13/30\n",
      "2000/2000 [==============================] - 197s 99ms/step - loss: 0.0548 - nmse_accuracy: -6.0626 - val_loss: 0.0627 - val_nmse_accuracy: -5.3775\n",
      "Epoch 14/30\n",
      "2000/2000 [==============================] - 220s 110ms/step - loss: 0.0526 - nmse_accuracy: -6.2250 - val_loss: 53148.3203 - val_nmse_accuracy: 52.3530\n",
      "Epoch 15/30\n",
      "2000/2000 [==============================] - 205s 103ms/step - loss: 0.0506 - nmse_accuracy: -6.4437 - val_loss: 4.0978 - val_nmse_accuracy: 11.6577\n",
      "Epoch 16/30\n",
      "2000/2000 [==============================] - 191s 95ms/step - loss: 0.0500 - nmse_accuracy: -6.4741 - val_loss: 0.0589 - val_nmse_accuracy: -5.6478\n",
      "Epoch 17/30\n",
      "2000/2000 [==============================] - 183s 92ms/step - loss: 0.0496 - nmse_accuracy: -6.4790 - val_loss: 0.0536 - val_nmse_accuracy: -6.1404\n",
      "Epoch 18/30\n",
      "2000/2000 [==============================] - 176s 88ms/step - loss: 0.0485 - nmse_accuracy: -6.6520 - val_loss: 0.5076 - val_nmse_accuracy: 2.5445\n",
      "Epoch 19/30\n",
      "2000/2000 [==============================] - 173s 86ms/step - loss: 0.0465 - nmse_accuracy: -6.8255 - val_loss: 736861837890420736.0000 - val_nmse_accuracy: 182.2990\n",
      "Epoch 20/30\n",
      "2000/2000 [==============================] - 200s 100ms/step - loss: 0.0460 - nmse_accuracy: -6.8446 - val_loss: 54.6098 - val_nmse_accuracy: 21.5962\n",
      "Epoch 21/30\n",
      "2000/2000 [==============================] - 183s 92ms/step - loss: 0.0457 - nmse_accuracy: -6.8625 - val_loss: 0.0892 - val_nmse_accuracy: -5.0013\n",
      "Epoch 22/30\n",
      "2000/2000 [==============================] - 180s 90ms/step - loss: 0.0453 - nmse_accuracy: -6.9109 - val_loss: 0.0513 - val_nmse_accuracy: -6.3945\n",
      "Epoch 23/30\n",
      "2000/2000 [==============================] - 177s 89ms/step - loss: 0.0451 - nmse_accuracy: -6.9516 - val_loss: 0.0534 - val_nmse_accuracy: -6.0687\n",
      "Epoch 24/30\n",
      "2000/2000 [==============================] - 188s 94ms/step - loss: 0.0449 - nmse_accuracy: -6.9686 - val_loss: 0.0512 - val_nmse_accuracy: -6.3267\n",
      "Epoch 25/30\n",
      "2000/2000 [==============================] - 192s 96ms/step - loss: 0.0444 - nmse_accuracy: -7.0093 - val_loss: 0.0502 - val_nmse_accuracy: -6.4690\n",
      "Epoch 26/30\n",
      "2000/2000 [==============================] - 178s 89ms/step - loss: 0.0444 - nmse_accuracy: -7.0454 - val_loss: 0.0499 - val_nmse_accuracy: -6.4753\n",
      "Epoch 27/30\n",
      "2000/2000 [==============================] - 150s 75ms/step - loss: 0.0442 - nmse_accuracy: -7.0369 - val_loss: 0.0506 - val_nmse_accuracy: -6.3555\n",
      "Epoch 28/30\n",
      "2000/2000 [==============================] - 118s 59ms/step - loss: 0.0435 - nmse_accuracy: -7.1259 - val_loss: 0.0500 - val_nmse_accuracy: -6.4027\n",
      "Epoch 29/30\n",
      "2000/2000 [==============================] - 118s 59ms/step - loss: 0.0437 - nmse_accuracy: -7.1044 - val_loss: 0.0480 - val_nmse_accuracy: -6.6217\n",
      "Epoch 30/30\n",
      "2000/2000 [==============================] - 115s 57ms/step - loss: 0.0435 - nmse_accuracy: -7.0977 - val_loss: 1457.5690 - val_nmse_accuracy: 36.6253\n",
      "313/313 - 1s - loss: 0.0523 - nmse_accuracy: -6.0237e+00\n",
      "Test loss: 0.052348677068948746\n",
      "Test accuracy: -6.023699760437012\n"
     ]
    }
   ],
   "source": [
    "decoder.compile(\n",
    "    loss=keras.losses.MeanSquaredError(),\n",
    "    optimizer=keras.optimizers.RMSprop(),\n",
    "    metrics=NMSE_Accuracy(),\n",
    ")\n",
    "\n",
    "history = decoder.fit(y_set, h_set, batch_size=1000, epochs=30, validation_split=0.2)\n",
    "\n",
    "test_scores = decoder.evaluate(y_test, h_test, verbose=2)\n",
    "print(\"Test loss:\", test_scores[0])\n",
    "print(\"Test accuracy:\", test_scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"estimator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bp1 (BatchNormalization)     (3, 50)                   200       \n",
      "_________________________________________________________________\n",
      "layer1 (Dense)               (3, 150)                  7650      \n",
      "_________________________________________________________________\n",
      "bp2 (BatchNormalization)     (3, 150)                  600       \n",
      "_________________________________________________________________\n",
      "layer2 (Dense)               (3, 150)                  22650     \n",
      "_________________________________________________________________\n",
      "bp3 (BatchNormalization)     (3, 150)                  600       \n",
      "_________________________________________________________________\n",
      "layer3 (Dense)               (3, 150)                  22650     \n",
      "_________________________________________________________________\n",
      "pb4 (BatchNormalization)     (3, 150)                  600       \n",
      "_________________________________________________________________\n",
      "layer4 (Dense)               (3, 100)                  15100     \n",
      "=================================================================\n",
      "Total params: 70,050\n",
      "Trainable params: 69,050\n",
      "Non-trainable params: 1,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "estimator = keras.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.BatchNormalization(name=\"bp1\"),\n",
    "        layers.Dense(user_number + code_length, activation=\"relu\", name=\"layer1\"),\n",
    "        tf.keras.layers.BatchNormalization(name=\"bp2\"),\n",
    "        layers.Dense(user_number + code_length, activation=\"relu\", name=\"layer2\"),\n",
    "        tf.keras.layers.BatchNormalization(name=\"bp3\"),\n",
    "        layers.Dense(user_number + code_length, activation=\"relu\", name=\"layer3\"),\n",
    "        tf.keras.layers.BatchNormalization(name=\"pb4\"),\n",
    "        layers.Dense(user_number, activation=\"relu\", name=\"layer4\"),\n",
    "    ],name = 'estimator'\n",
    ")\n",
    "# Call model on a test input\n",
    "y = tf.ones((3, code_length))\n",
    "x_hat = estimator(y)\n",
    "estimator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "2000/2000 [==============================] - 32s 15ms/step - loss: 0.0652 - nmse_accuracy: -5.9160 - val_loss: 0.0097 - val_nmse_accuracy: -13.9134\n",
      "Epoch 2/30\n",
      "2000/2000 [==============================] - 34s 17ms/step - loss: 0.0092 - nmse_accuracy: -14.1203 - val_loss: 0.0079 - val_nmse_accuracy: -14.8773\n",
      "Epoch 3/30\n",
      "2000/2000 [==============================] - 33s 17ms/step - loss: 0.0079 - nmse_accuracy: -14.8092 - val_loss: 0.0072 - val_nmse_accuracy: -15.2688\n",
      "Epoch 4/30\n",
      "2000/2000 [==============================] - 26s 13ms/step - loss: 0.0074 - nmse_accuracy: -15.1167 - val_loss: 0.0069 - val_nmse_accuracy: -15.4451\n",
      "Epoch 5/30\n",
      "2000/2000 [==============================] - 25s 13ms/step - loss: 0.0071 - nmse_accuracy: -15.3180 - val_loss: 0.0068 - val_nmse_accuracy: -15.5173\n",
      "Epoch 6/30\n",
      "2000/2000 [==============================] - 24s 12ms/step - loss: 0.0069 - nmse_accuracy: -15.4425 - val_loss: 0.0066 - val_nmse_accuracy: -15.6298\n",
      "Epoch 7/30\n",
      "2000/2000 [==============================] - 24s 12ms/step - loss: 0.0068 - nmse_accuracy: -15.5330 - val_loss: 0.0065 - val_nmse_accuracy: -15.7435\n",
      "Epoch 8/30\n",
      "2000/2000 [==============================] - 24s 12ms/step - loss: 0.0067 - nmse_accuracy: -15.5838 - val_loss: 0.0065 - val_nmse_accuracy: -15.7643\n",
      "Epoch 9/30\n",
      "2000/2000 [==============================] - 24s 12ms/step - loss: 0.0066 - nmse_accuracy: -15.5978 - val_loss: 0.0064 - val_nmse_accuracy: -15.8077\n",
      "Epoch 10/30\n",
      "2000/2000 [==============================] - 24s 12ms/step - loss: 0.0066 - nmse_accuracy: -15.6682 - val_loss: 0.0063 - val_nmse_accuracy: -15.8655\n",
      "Epoch 11/30\n",
      "2000/2000 [==============================] - 24s 12ms/step - loss: 0.0065 - nmse_accuracy: -15.6832 - val_loss: 0.0063 - val_nmse_accuracy: -15.8909\n",
      "Epoch 12/30\n",
      "2000/2000 [==============================] - 24s 12ms/step - loss: 0.0065 - nmse_accuracy: -15.7197 - val_loss: 0.0063 - val_nmse_accuracy: -15.8958\n",
      "Epoch 13/30\n",
      "2000/2000 [==============================] - 26s 13ms/step - loss: 0.0065 - nmse_accuracy: -15.7245 - val_loss: 0.0063 - val_nmse_accuracy: -15.8822\n",
      "Epoch 14/30\n",
      "2000/2000 [==============================] - 24s 12ms/step - loss: 0.0064 - nmse_accuracy: -15.7561 - val_loss: 0.0062 - val_nmse_accuracy: -15.9226\n",
      "Epoch 15/30\n",
      "2000/2000 [==============================] - 25s 12ms/step - loss: 0.0064 - nmse_accuracy: -15.7709 - val_loss: 0.0062 - val_nmse_accuracy: -15.9577\n",
      "Epoch 16/30\n",
      "2000/2000 [==============================] - 28s 14ms/step - loss: 0.0064 - nmse_accuracy: -15.7839 - val_loss: 0.0063 - val_nmse_accuracy: -15.9040\n",
      "Epoch 17/30\n",
      "2000/2000 [==============================] - 24s 12ms/step - loss: 0.0064 - nmse_accuracy: -15.7718 - val_loss: 0.0061 - val_nmse_accuracy: -16.0020\n",
      "Epoch 18/30\n",
      "2000/2000 [==============================] - 25s 12ms/step - loss: 0.0064 - nmse_accuracy: -15.8151 - val_loss: 0.0062 - val_nmse_accuracy: -15.9920\n",
      "Epoch 19/30\n",
      "2000/2000 [==============================] - 25s 13ms/step - loss: 0.0064 - nmse_accuracy: -15.7983 - val_loss: 0.0062 - val_nmse_accuracy: -15.9825\n",
      "Epoch 20/30\n",
      "2000/2000 [==============================] - 25s 12ms/step - loss: 0.0063 - nmse_accuracy: -15.8350 - val_loss: 0.0061 - val_nmse_accuracy: -16.0038\n",
      "Epoch 21/30\n",
      "2000/2000 [==============================] - 25s 12ms/step - loss: 0.0063 - nmse_accuracy: -15.8124 - val_loss: 0.0062 - val_nmse_accuracy: -15.9781\n",
      "Epoch 22/30\n",
      "2000/2000 [==============================] - 29s 15ms/step - loss: 0.0063 - nmse_accuracy: -15.8438 - val_loss: 0.0061 - val_nmse_accuracy: -16.0244\n",
      "Epoch 23/30\n",
      "2000/2000 [==============================] - 33s 16ms/step - loss: 0.0063 - nmse_accuracy: -15.8226 - val_loss: 0.0061 - val_nmse_accuracy: -16.0095\n",
      "Epoch 24/30\n",
      "2000/2000 [==============================] - 25s 13ms/step - loss: 0.0063 - nmse_accuracy: -15.8502 - val_loss: 0.0061 - val_nmse_accuracy: -16.0386\n",
      "Epoch 25/30\n",
      "2000/2000 [==============================] - 30s 15ms/step - loss: 0.0063 - nmse_accuracy: -15.8401 - val_loss: 0.0061 - val_nmse_accuracy: -16.0577\n",
      "Epoch 26/30\n",
      "2000/2000 [==============================] - 32s 16ms/step - loss: 0.0063 - nmse_accuracy: -15.8653 - val_loss: 0.0061 - val_nmse_accuracy: -16.0596\n",
      "Epoch 27/30\n",
      "2000/2000 [==============================] - 29s 14ms/step - loss: 0.0063 - nmse_accuracy: -15.8515 - val_loss: 0.0060 - val_nmse_accuracy: -16.0904\n",
      "Epoch 28/30\n",
      "2000/2000 [==============================] - 32s 16ms/step - loss: 0.0063 - nmse_accuracy: -15.8820 - val_loss: 0.0060 - val_nmse_accuracy: -16.0897\n",
      "Epoch 29/30\n",
      "2000/2000 [==============================] - 26s 13ms/step - loss: 0.0062 - nmse_accuracy: -15.8764 - val_loss: 0.0060 - val_nmse_accuracy: -16.0945\n",
      "Epoch 30/30\n",
      "2000/2000 [==============================] - 25s 12ms/step - loss: 0.0062 - nmse_accuracy: -15.9061 - val_loss: 0.0060 - val_nmse_accuracy: -16.1173\n",
      "313/313 - 1s - loss: 0.0060 - nmse_accuracy: -1.5961e+01\n",
      "Test loss: 0.006015625316649675\n",
      "Test accuracy: -15.960895538330078\n"
     ]
    }
   ],
   "source": [
    "estimator.compile(\n",
    "    loss=keras.losses.MeanSquaredError(),\n",
    "    optimizer=keras.optimizers.RMSprop(),\n",
    "    metrics=NMSE_Accuracy(),\n",
    ")\n",
    "\n",
    "history = estimator.fit(y_set, h_set, batch_size=1000, epochs=30, validation_split=0.2)\n",
    "\n",
    "test_scores = estimator.evaluate(y_test, h_test, verbose=2)\n",
    "print(\"Test loss:\", test_scores[0])\n",
    "print(\"Test accuracy:\", test_scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"estimator_mini\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bp1 (BatchNormalization)     (3, 50)                   200       \n",
      "_________________________________________________________________\n",
      "layer1 (Dense)               (3, 100)                  5100      \n",
      "_________________________________________________________________\n",
      "bp2 (BatchNormalization)     (3, 100)                  400       \n",
      "_________________________________________________________________\n",
      "layer2 (Dense)               (3, 100)                  10100     \n",
      "_________________________________________________________________\n",
      "bp3 (BatchNormalization)     (3, 100)                  400       \n",
      "_________________________________________________________________\n",
      "layer3 (Dense)               (3, 100)                  10100     \n",
      "_________________________________________________________________\n",
      "pb4 (BatchNormalization)     (3, 100)                  400       \n",
      "_________________________________________________________________\n",
      "layer4 (Dense)               (3, 100)                  10100     \n",
      "=================================================================\n",
      "Total params: 36,800\n",
      "Trainable params: 36,100\n",
      "Non-trainable params: 700\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "estimator_mini = keras.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.BatchNormalization(name=\"bp1\"),\n",
    "        layers.Dense(user_number, activation=\"relu\", name=\"layer1\"),\n",
    "        tf.keras.layers.BatchNormalization(name=\"bp2\"),\n",
    "        layers.Dense(user_number, activation=\"relu\", name=\"layer2\"),\n",
    "        tf.keras.layers.BatchNormalization(name=\"bp3\"),\n",
    "        layers.Dense(user_number, activation=\"relu\", name=\"layer3\"),\n",
    "        tf.keras.layers.BatchNormalization(name=\"pb4\"),\n",
    "        layers.Dense(user_number, activation=\"relu\", name=\"layer4\"),\n",
    "    ],name = 'estimator_mini'\n",
    ")\n",
    "# Call model on a test input\n",
    "y = tf.ones((3, code_length))\n",
    "x_hat = estimator_mini(y)\n",
    "estimator_mini.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "2000/2000 [==============================] - 20s 9ms/step - loss: 0.0791 - nmse_accuracy: -4.8947 - val_loss: 0.0163 - val_nmse_accuracy: -11.5169\n",
      "Epoch 2/30\n",
      "2000/2000 [==============================] - 20s 10ms/step - loss: 0.0150 - nmse_accuracy: -11.8580 - val_loss: 0.0132 - val_nmse_accuracy: -12.4799\n",
      "Epoch 3/30\n",
      "2000/2000 [==============================] - 18s 9ms/step - loss: 0.0132 - nmse_accuracy: -12.4430 - val_loss: 0.0123 - val_nmse_accuracy: -12.7854\n",
      "Epoch 4/30\n",
      "2000/2000 [==============================] - 16s 8ms/step - loss: 0.0125 - nmse_accuracy: -12.7141 - val_loss: 0.0121 - val_nmse_accuracy: -12.8745\n",
      "Epoch 5/30\n",
      "2000/2000 [==============================] - 16s 8ms/step - loss: 0.0123 - nmse_accuracy: -12.7674 - val_loss: 0.0120 - val_nmse_accuracy: -12.9188\n",
      "Epoch 6/30\n",
      "2000/2000 [==============================] - 16s 8ms/step - loss: 0.0122 - nmse_accuracy: -12.8157 - val_loss: 0.0118 - val_nmse_accuracy: -13.0076\n",
      "Epoch 7/30\n",
      "2000/2000 [==============================] - 16s 8ms/step - loss: 0.0120 - nmse_accuracy: -12.8715 - val_loss: 0.0117 - val_nmse_accuracy: -13.0221\n",
      "Epoch 8/30\n",
      "2000/2000 [==============================] - 16s 8ms/step - loss: 0.0120 - nmse_accuracy: -12.8877 - val_loss: 0.0116 - val_nmse_accuracy: -13.0726\n",
      "Epoch 9/30\n",
      "2000/2000 [==============================] - 21s 10ms/step - loss: 0.0118 - nmse_accuracy: -12.9442 - val_loss: 0.0115 - val_nmse_accuracy: -13.1010\n",
      "Epoch 10/30\n",
      "2000/2000 [==============================] - 18s 9ms/step - loss: 0.0118 - nmse_accuracy: -12.9470 - val_loss: 0.0115 - val_nmse_accuracy: -13.1097\n",
      "Epoch 11/30\n",
      "2000/2000 [==============================] - 22s 11ms/step - loss: 0.0118 - nmse_accuracy: -12.9891 - val_loss: 0.0115 - val_nmse_accuracy: -13.1383\n",
      "Epoch 12/30\n",
      "2000/2000 [==============================] - 22s 11ms/step - loss: 0.0117 - nmse_accuracy: -12.9814 - val_loss: 0.0115 - val_nmse_accuracy: -13.1330\n",
      "Epoch 13/30\n",
      "2000/2000 [==============================] - 20s 10ms/step - loss: 0.0117 - nmse_accuracy: -12.9702 - val_loss: 0.0115 - val_nmse_accuracy: -13.1412\n",
      "Epoch 14/30\n",
      "2000/2000 [==============================] - 21s 11ms/step - loss: 0.0117 - nmse_accuracy: -13.0047 - val_loss: 0.0114 - val_nmse_accuracy: -13.1448\n",
      "Epoch 15/30\n",
      "2000/2000 [==============================] - 17s 8ms/step - loss: 0.0117 - nmse_accuracy: -12.9883 - val_loss: 0.0114 - val_nmse_accuracy: -13.1645\n",
      "Epoch 16/30\n",
      "2000/2000 [==============================] - 17s 9ms/step - loss: 0.0117 - nmse_accuracy: -13.0215 - val_loss: 0.0114 - val_nmse_accuracy: -13.1654\n",
      "Epoch 17/30\n",
      "2000/2000 [==============================] - 18s 9ms/step - loss: 0.0117 - nmse_accuracy: -13.0359 - val_loss: 0.0114 - val_nmse_accuracy: -13.1522\n",
      "Epoch 18/30\n",
      "2000/2000 [==============================] - 17s 9ms/step - loss: 0.0117 - nmse_accuracy: -13.0508 - val_loss: 0.0114 - val_nmse_accuracy: -13.1647\n",
      "Epoch 19/30\n",
      "2000/2000 [==============================] - 17s 9ms/step - loss: 0.0116 - nmse_accuracy: -inf - val_loss: 0.0112 - val_nmse_accuracy: -13.2252\n",
      "Epoch 20/30\n",
      "2000/2000 [==============================] - 17s 8ms/step - loss: 0.0115 - nmse_accuracy: -13.0632 - val_loss: 0.0113 - val_nmse_accuracy: -13.2160\n",
      "Epoch 21/30\n",
      "2000/2000 [==============================] - 16s 8ms/step - loss: 0.0115 - nmse_accuracy: -13.1428 - val_loss: 0.0112 - val_nmse_accuracy: -13.2469\n",
      "Epoch 22/30\n",
      "2000/2000 [==============================] - 18s 9ms/step - loss: 0.0115 - nmse_accuracy: -13.0685 - val_loss: 0.0112 - val_nmse_accuracy: -13.2541\n",
      "Epoch 23/30\n",
      "2000/2000 [==============================] - 18s 9ms/step - loss: 0.0115 - nmse_accuracy: -13.0997 - val_loss: 0.0112 - val_nmse_accuracy: -13.2460\n",
      "Epoch 24/30\n",
      "2000/2000 [==============================] - 18s 9ms/step - loss: 0.0115 - nmse_accuracy: -13.0861 - val_loss: 0.0112 - val_nmse_accuracy: -13.2529\n",
      "Epoch 25/30\n",
      "2000/2000 [==============================] - 19s 9ms/step - loss: 0.0115 - nmse_accuracy: -13.1220 - val_loss: 0.0112 - val_nmse_accuracy: -13.2517\n",
      "Epoch 26/30\n",
      "2000/2000 [==============================] - 18s 9ms/step - loss: 0.0114 - nmse_accuracy: -13.1047 - val_loss: 0.0112 - val_nmse_accuracy: -13.2532\n",
      "Epoch 27/30\n",
      "2000/2000 [==============================] - 16s 8ms/step - loss: 0.0114 - nmse_accuracy: -13.1117 - val_loss: 0.0111 - val_nmse_accuracy: -13.2716\n",
      "Epoch 28/30\n",
      "2000/2000 [==============================] - 17s 8ms/step - loss: 0.0115 - nmse_accuracy: -13.0684 - val_loss: 0.0111 - val_nmse_accuracy: -13.2731\n",
      "Epoch 29/30\n",
      "2000/2000 [==============================] - 16s 8ms/step - loss: 0.0114 - nmse_accuracy: -13.0681 - val_loss: 0.0111 - val_nmse_accuracy: -13.2685\n",
      "Epoch 30/30\n",
      "2000/2000 [==============================] - 19s 9ms/step - loss: 0.0114 - nmse_accuracy: -13.1001 - val_loss: 0.0111 - val_nmse_accuracy: -13.2724\n",
      "313/313 - 1s - loss: 0.0110 - nmse_accuracy: -1.3152e+01\n",
      "Test loss: 0.011029518209397793\n",
      "Test accuracy: -13.15235710144043\n"
     ]
    }
   ],
   "source": [
    "estimator_mini.compile(\n",
    "    loss=keras.losses.MeanSquaredError(),\n",
    "    optimizer=keras.optimizers.RMSprop(),\n",
    "    metrics=NMSE_Accuracy(),\n",
    ")\n",
    "\n",
    "history = estimator_mini.fit(y_set, h_set, batch_size=1000, epochs=30, validation_split=0.2)\n",
    "\n",
    "test_scores = estimator_mini.evaluate(y_test, h_test, verbose=2)\n",
    "print(\"Test loss:\", test_scores[0])\n",
    "print(\"Test accuracy:\", test_scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"estimator_big\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bp1 (BatchNormalization)     (3, 50)                   200       \n",
      "_________________________________________________________________\n",
      "layer1 (Dense)               (3, 200)                  10200     \n",
      "_________________________________________________________________\n",
      "bp2 (BatchNormalization)     (3, 200)                  800       \n",
      "_________________________________________________________________\n",
      "layer2 (Dense)               (3, 200)                  40200     \n",
      "_________________________________________________________________\n",
      "bp3 (BatchNormalization)     (3, 200)                  800       \n",
      "_________________________________________________________________\n",
      "layer3 (Dense)               (3, 200)                  40200     \n",
      "_________________________________________________________________\n",
      "pb4 (BatchNormalization)     (3, 200)                  800       \n",
      "_________________________________________________________________\n",
      "layer4 (Dense)               (3, 100)                  20100     \n",
      "=================================================================\n",
      "Total params: 113,300\n",
      "Trainable params: 112,000\n",
      "Non-trainable params: 1,300\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "estimator_big = keras.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.BatchNormalization(name=\"bp1\"),\n",
    "        layers.Dense(user_number*2, activation=\"relu\", name=\"layer1\"),\n",
    "        tf.keras.layers.BatchNormalization(name=\"bp2\"),\n",
    "        layers.Dense(user_number*2, activation=\"relu\", name=\"layer2\"),\n",
    "        tf.keras.layers.BatchNormalization(name=\"bp3\"),\n",
    "        layers.Dense(user_number*2, activation=\"relu\", name=\"layer3\"),\n",
    "        tf.keras.layers.BatchNormalization(name=\"pb4\"),\n",
    "        layers.Dense(user_number, activation=\"relu\", name=\"layer4\"),\n",
    "    ],name = 'estimator_big'\n",
    ")\n",
    "# Call model on a test input\n",
    "y = tf.ones((3, code_length))\n",
    "x_hat = estimator_big(y)\n",
    "estimator_big.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "2000/2000 [==============================] - 35s 17ms/step - loss: 0.0587 - nmse_accuracy: -6.4189 - val_loss: 0.0082 - val_nmse_accuracy: -14.6806\n",
      "Epoch 2/30\n",
      "2000/2000 [==============================] - 31s 16ms/step - loss: 0.0076 - nmse_accuracy: -14.9916 - val_loss: 0.0062 - val_nmse_accuracy: -15.9025\n",
      "Epoch 3/30\n",
      "2000/2000 [==============================] - 33s 16ms/step - loss: 0.0061 - nmse_accuracy: -15.9682 - val_loss: 0.0055 - val_nmse_accuracy: -16.4617\n",
      "Epoch 4/30\n",
      "2000/2000 [==============================] - 31s 16ms/step - loss: 0.0056 - nmse_accuracy: -16.3830 - val_loss: 0.0052 - val_nmse_accuracy: -16.7763\n",
      "Epoch 5/30\n",
      "2000/2000 [==============================] - 36s 18ms/step - loss: 0.0052 - nmse_accuracy: -16.6506 - val_loss: 0.0049 - val_nmse_accuracy: -16.9725\n",
      "Epoch 6/30\n",
      "2000/2000 [==============================] - 32s 16ms/step - loss: 0.0050 - nmse_accuracy: -16.8192 - val_loss: 0.0048 - val_nmse_accuracy: -17.1275\n",
      "Epoch 7/30\n",
      "2000/2000 [==============================] - 32s 16ms/step - loss: 0.0049 - nmse_accuracy: -17.0037 - val_loss: 0.0046 - val_nmse_accuracy: -17.3204\n",
      "Epoch 8/30\n",
      "2000/2000 [==============================] - 33s 16ms/step - loss: 0.0048 - nmse_accuracy: -17.1173 - val_loss: 0.0046 - val_nmse_accuracy: -17.3115\n",
      "Epoch 9/30\n",
      "2000/2000 [==============================] - 39s 19ms/step - loss: 0.0047 - nmse_accuracy: -17.1690 - val_loss: 0.0044 - val_nmse_accuracy: -17.4708\n",
      "Epoch 10/30\n",
      "2000/2000 [==============================] - 42s 21ms/step - loss: 0.0046 - nmse_accuracy: -17.2813 - val_loss: 0.0044 - val_nmse_accuracy: -17.5456\n",
      "Epoch 11/30\n",
      "2000/2000 [==============================] - 33s 17ms/step - loss: 0.0045 - nmse_accuracy: -17.3423 - val_loss: 0.0044 - val_nmse_accuracy: -17.5542\n",
      "Epoch 12/30\n",
      "2000/2000 [==============================] - 33s 17ms/step - loss: 0.0045 - nmse_accuracy: -17.3925 - val_loss: 0.0043 - val_nmse_accuracy: -17.6591\n",
      "Epoch 13/30\n",
      "2000/2000 [==============================] - 33s 17ms/step - loss: 0.0045 - nmse_accuracy: -inf - val_loss: 0.0042 - val_nmse_accuracy: -17.6936\n",
      "Epoch 14/30\n",
      "2000/2000 [==============================] - 39s 19ms/step - loss: 0.0044 - nmse_accuracy: -17.4377 - val_loss: 0.0042 - val_nmse_accuracy: -17.6962\n",
      "Epoch 15/30\n",
      "2000/2000 [==============================] - 35s 17ms/step - loss: 0.0044 - nmse_accuracy: -17.4955 - val_loss: 0.0042 - val_nmse_accuracy: -17.7573\n",
      "Epoch 16/30\n",
      "2000/2000 [==============================] - 33s 17ms/step - loss: 0.0043 - nmse_accuracy: -17.5302 - val_loss: 0.0041 - val_nmse_accuracy: -17.8190\n",
      "Epoch 17/30\n",
      "2000/2000 [==============================] - 34s 17ms/step - loss: 0.0043 - nmse_accuracy: -17.5433 - val_loss: 0.0042 - val_nmse_accuracy: -17.7952\n",
      "Epoch 18/30\n",
      "2000/2000 [==============================] - 35s 18ms/step - loss: 0.0043 - nmse_accuracy: -17.5589 - val_loss: 0.0042 - val_nmse_accuracy: -17.7469\n",
      "Epoch 19/30\n",
      "2000/2000 [==============================] - 46s 23ms/step - loss: 0.0043 - nmse_accuracy: -17.5950 - val_loss: 0.0041 - val_nmse_accuracy: -17.8226\n",
      "Epoch 20/30\n",
      "2000/2000 [==============================] - 37s 18ms/step - loss: 0.0043 - nmse_accuracy: -17.5711 - val_loss: 0.0041 - val_nmse_accuracy: -17.9094\n",
      "Epoch 21/30\n",
      "2000/2000 [==============================] - 42s 21ms/step - loss: 0.0043 - nmse_accuracy: -17.5944 - val_loss: 0.0041 - val_nmse_accuracy: -17.9031\n",
      "Epoch 22/30\n",
      "2000/2000 [==============================] - 37s 18ms/step - loss: 0.0043 - nmse_accuracy: -17.6353 - val_loss: 0.0041 - val_nmse_accuracy: -17.8800\n",
      "Epoch 23/30\n",
      "2000/2000 [==============================] - 35s 18ms/step - loss: 0.0042 - nmse_accuracy: -17.6357 - val_loss: 0.0041 - val_nmse_accuracy: -17.8941\n",
      "Epoch 24/30\n",
      "2000/2000 [==============================] - 35s 17ms/step - loss: 0.0042 - nmse_accuracy: -17.6812 - val_loss: 0.0040 - val_nmse_accuracy: -17.9432\n",
      "Epoch 25/30\n",
      "2000/2000 [==============================] - 36s 18ms/step - loss: 0.0042 - nmse_accuracy: -17.7039 - val_loss: 0.0040 - val_nmse_accuracy: -17.9211\n",
      "Epoch 26/30\n",
      "2000/2000 [==============================] - 35s 17ms/step - loss: 0.0042 - nmse_accuracy: -17.6943 - val_loss: 0.0040 - val_nmse_accuracy: -17.9714\n",
      "Epoch 27/30\n",
      "2000/2000 [==============================] - 35s 17ms/step - loss: 0.0042 - nmse_accuracy: -17.6797 - val_loss: 0.0040 - val_nmse_accuracy: -17.9668\n",
      "Epoch 28/30\n",
      "2000/2000 [==============================] - 37s 19ms/step - loss: 0.0042 - nmse_accuracy: -17.7242 - val_loss: 0.0040 - val_nmse_accuracy: -17.9721\n",
      "Epoch 29/30\n",
      "2000/2000 [==============================] - 36s 18ms/step - loss: 0.0042 - nmse_accuracy: -17.7293 - val_loss: 0.0040 - val_nmse_accuracy: -18.0022\n",
      "Epoch 30/30\n",
      "2000/2000 [==============================] - 38s 19ms/step - loss: 0.0042 - nmse_accuracy: -17.7408 - val_loss: 0.0040 - val_nmse_accuracy: -18.0234\n",
      "313/313 - 1s - loss: 0.0040 - nmse_accuracy: -1.7854e+01\n",
      "Test loss: 0.00398174487054348\n",
      "Test accuracy: -17.85357666015625\n"
     ]
    }
   ],
   "source": [
    "estimator_big.compile(\n",
    "    loss=keras.losses.MeanSquaredError(),\n",
    "    optimizer=keras.optimizers.RMSprop(),\n",
    "    metrics=NMSE_Accuracy(),\n",
    ")\n",
    "\n",
    "history = estimator_big.fit(y_set, h_set, batch_size=1000, epochs=30, validation_split=0.2)\n",
    "\n",
    "test_scores = estimator_big.evaluate(y_test, h_test, verbose=2)\n",
    "print(\"Test loss:\", test_scores[0])\n",
    "print(\"Test accuracy:\", test_scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bp1 (BatchNormalization)     (3, 50)                   200       \n",
      "_________________________________________________________________\n",
      "layer1 (Dense)               (3, 150)                  7650      \n",
      "_________________________________________________________________\n",
      "bp2 (BatchNormalization)     (3, 150)                  600       \n",
      "_________________________________________________________________\n",
      "layer2 (Dense)               (3, 150)                  22650     \n",
      "_________________________________________________________________\n",
      "bp3 (BatchNormalization)     (3, 150)                  600       \n",
      "_________________________________________________________________\n",
      "layer3 (Dense)               (3, 150)                  22650     \n",
      "_________________________________________________________________\n",
      "pb4 (BatchNormalization)     (3, 150)                  600       \n",
      "_________________________________________________________________\n",
      "layer4 (Dense)               (3, 100)                  15100     \n",
      "_________________________________________________________________\n",
      "bp1_2 (BatchNormalization)   (3, 100)                  400       \n",
      "_________________________________________________________________\n",
      "layer1_2 (Dense)             (3, 150)                  15150     \n",
      "_________________________________________________________________\n",
      "bp2_2 (BatchNormalization)   (3, 150)                  600       \n",
      "_________________________________________________________________\n",
      "layer2_2 (Dense)             (3, 150)                  22650     \n",
      "_________________________________________________________________\n",
      "bp3_2 (BatchNormalization)   (3, 150)                  600       \n",
      "_________________________________________________________________\n",
      "layer3_2 (Dense)             (3, 150)                  22650     \n",
      "_________________________________________________________________\n",
      "pb4_2 (BatchNormalization)   (3, 150)                  600       \n",
      "_________________________________________________________________\n",
      "layer4_2 (Dense)             (3, 100)                  15100     \n",
      "_________________________________________________________________\n",
      "bp1_3 (BatchNormalization)   (3, 100)                  400       \n",
      "_________________________________________________________________\n",
      "layer1_3 (Dense)             (3, 150)                  15150     \n",
      "_________________________________________________________________\n",
      "bp2_3 (BatchNormalization)   (3, 150)                  600       \n",
      "_________________________________________________________________\n",
      "layer2_3 (Dense)             (3, 150)                  22650     \n",
      "_________________________________________________________________\n",
      "bp3_3 (BatchNormalization)   (3, 150)                  600       \n",
      "_________________________________________________________________\n",
      "layer3_3 (Dense)             (3, 150)                  22650     \n",
      "_________________________________________________________________\n",
      "pb4_3 (BatchNormalization)   (3, 150)                  600       \n",
      "_________________________________________________________________\n",
      "layer4_3 (Dense)             (3, 100)                  15100     \n",
      "_________________________________________________________________\n",
      "bp1_4 (BatchNormalization)   (3, 100)                  400       \n",
      "_________________________________________________________________\n",
      "layer1_4 (Dense)             (3, 150)                  15150     \n",
      "_________________________________________________________________\n",
      "bp2_4 (BatchNormalization)   (3, 150)                  600       \n",
      "_________________________________________________________________\n",
      "layer2_4 (Dense)             (3, 150)                  22650     \n",
      "_________________________________________________________________\n",
      "bp3_4 (BatchNormalization)   (3, 150)                  600       \n",
      "_________________________________________________________________\n",
      "layer3_4 (Dense)             (3, 150)                  22650     \n",
      "_________________________________________________________________\n",
      "pb4_4 (BatchNormalization)   (3, 150)                  600       \n",
      "_________________________________________________________________\n",
      "layer4_4 (Dense)             (3, 100)                  15100     \n",
      "_________________________________________________________________\n",
      "bp1_5 (BatchNormalization)   (3, 100)                  400       \n",
      "_________________________________________________________________\n",
      "layer1_5 (Dense)             (3, 150)                  15150     \n",
      "_________________________________________________________________\n",
      "bp2_5 (BatchNormalization)   (3, 150)                  600       \n",
      "_________________________________________________________________\n",
      "layer2_5 (Dense)             (3, 150)                  22650     \n",
      "_________________________________________________________________\n",
      "bp3_5 (BatchNormalization)   (3, 150)                  600       \n",
      "_________________________________________________________________\n",
      "layer3_5 (Dense)             (3, 150)                  22650     \n",
      "_________________________________________________________________\n",
      "pb4_5 (BatchNormalization)   (3, 150)                  600       \n",
      "_________________________________________________________________\n",
      "layer4_5 (Dense)             (3, 100)                  15100     \n",
      "=================================================================\n",
      "Total params: 381,050\n",
      "Trainable params: 375,650\n",
      "Non-trainable params: 5,400\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "decoder_norm_loss = keras.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.BatchNormalization(name=\"bp1\"),\n",
    "        layers.Dense(user_number + code_length, activation=\"relu\", name=\"layer1\"),\n",
    "        tf.keras.layers.BatchNormalization(name=\"bp2\"),\n",
    "        layers.Dense(user_number + code_length, activation=\"relu\", name=\"layer2\"),\n",
    "        tf.keras.layers.BatchNormalization(name=\"bp3\"),\n",
    "        layers.Dense(user_number + code_length, activation=\"relu\", name=\"layer3\"),\n",
    "        tf.keras.layers.BatchNormalization(name=\"pb4\"),\n",
    "        layers.Dense(user_number, activation=\"relu\", name=\"layer4\"),\n",
    "        tf.keras.layers.BatchNormalization(name=\"bp1_2\"),\n",
    "        layers.Dense(user_number + code_length, activation=\"relu\", name=\"layer1_2\"),\n",
    "        tf.keras.layers.BatchNormalization(name=\"bp2_2\"),\n",
    "        layers.Dense(user_number + code_length, activation=\"relu\", name=\"layer2_2\"),\n",
    "        tf.keras.layers.BatchNormalization(name=\"bp3_2\"),\n",
    "        layers.Dense(user_number + code_length, activation=\"relu\", name=\"layer3_2\"),\n",
    "        tf.keras.layers.BatchNormalization(name=\"pb4_2\"),\n",
    "        layers.Dense(user_number, activation=\"sigmoid\", name=\"layer4_2\"),\n",
    "        tf.keras.layers.BatchNormalization(name=\"bp1_3\"),\n",
    "        layers.Dense(user_number + code_length, activation=\"relu\", name=\"layer1_3\"),\n",
    "        tf.keras.layers.BatchNormalization(name=\"bp2_3\"),\n",
    "        layers.Dense(user_number + code_length, activation=\"relu\", name=\"layer2_3\"),\n",
    "        tf.keras.layers.BatchNormalization(name=\"bp3_3\"),\n",
    "        layers.Dense(user_number + code_length, activation=\"relu\", name=\"layer3_3\"),\n",
    "        tf.keras.layers.BatchNormalization(name=\"pb4_3\"),\n",
    "        layers.Dense(user_number, activation=\"relu\", name=\"layer4_3\"),\n",
    "        tf.keras.layers.BatchNormalization(name=\"bp1_4\"),\n",
    "        layers.Dense(user_number + code_length, activation=\"relu\", name=\"layer1_4\"),\n",
    "        tf.keras.layers.BatchNormalization(name=\"bp2_4\"),\n",
    "        layers.Dense(user_number + code_length, activation=\"relu\", name=\"layer2_4\"),\n",
    "        tf.keras.layers.BatchNormalization(name=\"bp3_4\"),\n",
    "        layers.Dense(user_number + code_length, activation=\"relu\", name=\"layer3_4\"),\n",
    "        tf.keras.layers.BatchNormalization(name=\"pb4_4\"),\n",
    "        layers.Dense(user_number, activation=\"sigmoid\", name=\"layer4_4\"),\n",
    "        tf.keras.layers.BatchNormalization(name=\"bp1_5\"),\n",
    "        layers.Dense(user_number + code_length, activation=\"relu\", name=\"layer1_5\"),\n",
    "        tf.keras.layers.BatchNormalization(name=\"bp2_5\"),\n",
    "        layers.Dense(user_number + code_length, activation=\"relu\", name=\"layer2_5\"),\n",
    "        tf.keras.layers.BatchNormalization(name=\"bp3_5\"),\n",
    "        layers.Dense(user_number + code_length, activation=\"relu\", name=\"layer3_5\"),\n",
    "        tf.keras.layers.BatchNormalization(name=\"pb4_5\"),\n",
    "        layers.Dense(user_number, activation=\"relu\", name=\"layer4_5\"),\n",
    "    ],name = 'decoder'\n",
    ")\n",
    "# Call model on a test input\n",
    "y = tf.ones((3, code_length))\n",
    "x_hat = decoder_norm_loss(y)\n",
    "decoder_norm_loss.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "2000/2000 [==============================] - 151s 71ms/step - loss: 0.1680 - nmse_accuracy: -0.8516 - val_loss: 0.1305 - val_nmse_accuracy: -1.9805\n",
      "Epoch 2/30\n",
      "2000/2000 [==============================] - 124s 62ms/step - loss: 0.1112 - nmse_accuracy: -2.7178 - val_loss: 0.1047 - val_nmse_accuracy: -3.0463\n",
      "Epoch 3/30\n",
      "2000/2000 [==============================] - 133s 66ms/step - loss: 0.1001 - nmse_accuracy: -3.2248 - val_loss: 0.0973 - val_nmse_accuracy: -3.3773\n",
      "Epoch 4/30\n",
      "2000/2000 [==============================] - 122s 61ms/step - loss: 0.0903 - nmse_accuracy: -3.7225 - val_loss: 0.0873 - val_nmse_accuracy: -3.9096\n",
      "Epoch 5/30\n",
      "2000/2000 [==============================] - 132s 66ms/step - loss: 0.0823 - nmse_accuracy: -4.1741 - val_loss: 0.0825 - val_nmse_accuracy: -4.1569\n",
      "Epoch 6/30\n",
      "2000/2000 [==============================] - 125s 62ms/step - loss: 0.0729 - nmse_accuracy: -4.7359 - val_loss: 0.0692 - val_nmse_accuracy: -5.0183\n",
      "Epoch 7/30\n",
      "2000/2000 [==============================] - 132s 66ms/step - loss: 0.0635 - nmse_accuracy: -5.4028 - val_loss: 0.0671 - val_nmse_accuracy: -5.0516\n",
      "Epoch 8/30\n",
      "2000/2000 [==============================] - 126s 63ms/step - loss: 0.0571 - nmse_accuracy: -5.9129 - val_loss: 0.0735 - val_nmse_accuracy: -4.6480\n",
      "Epoch 9/30\n",
      "2000/2000 [==============================] - 126s 63ms/step - loss: 0.0528 - nmse_accuracy: -6.2614 - val_loss: 0.0531 - val_nmse_accuracy: -6.2357\n",
      "Epoch 10/30\n",
      "2000/2000 [==============================] - 128s 64ms/step - loss: 0.0470 - nmse_accuracy: -6.8117 - val_loss: 0.0501 - val_nmse_accuracy: -6.4766\n",
      "Epoch 11/30\n",
      "2000/2000 [==============================] - 130s 65ms/step - loss: 0.0424 - nmse_accuracy: -7.2826 - val_loss: 0.0444 - val_nmse_accuracy: -7.0466\n",
      "Epoch 12/30\n",
      "2000/2000 [==============================] - 128s 64ms/step - loss: 0.0399 - nmse_accuracy: -7.5668 - val_loss: 0.0433 - val_nmse_accuracy: -7.1645\n",
      "Epoch 13/30\n",
      "2000/2000 [==============================] - 126s 63ms/step - loss: 0.0379 - nmse_accuracy: -7.7740 - val_loss: 0.0403 - val_nmse_accuracy: -7.5529\n",
      "Epoch 14/30\n",
      "2000/2000 [==============================] - 126s 63ms/step - loss: 0.0362 - nmse_accuracy: -8.0215 - val_loss: 0.0422 - val_nmse_accuracy: -7.2385\n",
      "Epoch 15/30\n",
      "2000/2000 [==============================] - 126s 63ms/step - loss: 0.0352 - nmse_accuracy: -8.1323 - val_loss: 0.0395 - val_nmse_accuracy: -7.5406\n",
      "Epoch 16/30\n",
      "2000/2000 [==============================] - 126s 63ms/step - loss: 0.0336 - nmse_accuracy: -8.3295 - val_loss: 0.0426 - val_nmse_accuracy: -7.0891\n",
      "Epoch 17/30\n",
      "2000/2000 [==============================] - 136s 68ms/step - loss: 0.0322 - nmse_accuracy: -8.5418 - val_loss: 0.0356 - val_nmse_accuracy: -8.0179\n",
      "Epoch 18/30\n",
      "2000/2000 [==============================] - 118s 59ms/step - loss: 0.0315 - nmse_accuracy: -8.6615 - val_loss: 11.2653 - val_nmse_accuracy: 13.2424\n",
      "Epoch 19/30\n",
      "2000/2000 [==============================] - 137s 69ms/step - loss: 0.0309 - nmse_accuracy: -8.7072 - val_loss: 0.0423 - val_nmse_accuracy: -7.7482\n",
      "Epoch 20/30\n",
      "2000/2000 [==============================] - 129s 64ms/step - loss: 0.0291 - nmse_accuracy: -9.0160 - val_loss: 14.3125 - val_nmse_accuracy: 14.2756\n",
      "Epoch 21/30\n",
      "2000/2000 [==============================] - 131s 65ms/step - loss: 0.0282 - nmse_accuracy: -9.1522 - val_loss: 0.0316 - val_nmse_accuracy: -8.5615\n",
      "Epoch 22/30\n",
      "2000/2000 [==============================] - 126s 63ms/step - loss: 0.0273 - nmse_accuracy: -9.3067 - val_loss: 0.0294 - val_nmse_accuracy: -8.8938\n",
      "Epoch 23/30\n",
      "2000/2000 [==============================] - 127s 63ms/step - loss: 0.0263 - nmse_accuracy: -9.4748 - val_loss: 1.0220 - val_nmse_accuracy: 3.5291\n",
      "Epoch 24/30\n",
      "2000/2000 [==============================] - 128s 64ms/step - loss: 0.0254 - nmse_accuracy: -9.6316 - val_loss: 0.7767 - val_nmse_accuracy: 2.3521\n",
      "Epoch 25/30\n",
      "2000/2000 [==============================] - 126s 63ms/step - loss: 0.0249 - nmse_accuracy: -9.7364 - val_loss: 1.1534 - val_nmse_accuracy: 4.3651\n",
      "Epoch 26/30\n",
      "2000/2000 [==============================] - 122s 61ms/step - loss: 0.0246 - nmse_accuracy: -9.7704 - val_loss: 1.8180 - val_nmse_accuracy: 5.9617\n",
      "Epoch 27/30\n",
      "2000/2000 [==============================] - 123s 61ms/step - loss: 0.0243 - nmse_accuracy: -9.8400 - val_loss: 0.0491 - val_nmse_accuracy: -7.6788\n",
      "Epoch 28/30\n",
      "2000/2000 [==============================] - 131s 66ms/step - loss: 0.0243 - nmse_accuracy: -9.8412 - val_loss: 15.7000 - val_nmse_accuracy: 15.6283\n",
      "Epoch 29/30\n",
      "2000/2000 [==============================] - 148s 74ms/step - loss: 0.0237 - nmse_accuracy: -9.9588 - val_loss: 49.2292 - val_nmse_accuracy: 20.2222\n",
      "Epoch 30/30\n",
      "2000/2000 [==============================] - 130s 65ms/step - loss: 0.0222 - nmse_accuracy: -10.2818 - val_loss: 0.0259 - val_nmse_accuracy: -9.3741\n",
      "313/313 - 1s - loss: 0.0259 - nmse_accuracy: -9.2333e+00\n",
      "Test loss: 0.025945009663701057\n",
      "Test accuracy: -9.233345985412598\n"
     ]
    }
   ],
   "source": [
    "decoder_norm_loss.compile(\n",
    "    loss=keras.losses.MeanSquaredError(),\n",
    "    optimizer=keras.optimizers.RMSprop(),\n",
    "    metrics=NMSE_Accuracy(),\n",
    ")\n",
    "\n",
    "history = decoder_norm_loss.fit(y_set, h_set, batch_size=1000, epochs=30, validation_split=0.2)\n",
    "\n",
    "test_scores = decoder_norm_loss.evaluate(y_test, h_test, verbose=2)\n",
    "print(\"Test loss:\", test_scores[0])\n",
    "print(\"Test accuracy:\", test_scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bp1 (BatchNormalization)     (3, 50)                   200       \n",
      "_________________________________________________________________\n",
      "layer1 (Dense)               (3, 150)                  7650      \n",
      "_________________________________________________________________\n",
      "bp2 (BatchNormalization)     (3, 150)                  600       \n",
      "_________________________________________________________________\n",
      "layer2 (Dense)               (3, 150)                  22650     \n",
      "_________________________________________________________________\n",
      "bp3 (BatchNormalization)     (3, 150)                  600       \n",
      "_________________________________________________________________\n",
      "layer3 (Dense)               (3, 150)                  22650     \n",
      "_________________________________________________________________\n",
      "pb4 (BatchNormalization)     (3, 150)                  600       \n",
      "_________________________________________________________________\n",
      "layer4 (Dense)               (3, 100)                  15100     \n",
      "_________________________________________________________________\n",
      "bp1_2 (BatchNormalization)   (3, 100)                  400       \n",
      "_________________________________________________________________\n",
      "layer1_2 (Dense)             (3, 150)                  15150     \n",
      "_________________________________________________________________\n",
      "bp2_2 (BatchNormalization)   (3, 150)                  600       \n",
      "_________________________________________________________________\n",
      "layer2_2 (Dense)             (3, 150)                  22650     \n",
      "_________________________________________________________________\n",
      "bp3_2 (BatchNormalization)   (3, 150)                  600       \n",
      "_________________________________________________________________\n",
      "layer3_2 (Dense)             (3, 150)                  22650     \n",
      "_________________________________________________________________\n",
      "pb4_2 (BatchNormalization)   (3, 150)                  600       \n",
      "_________________________________________________________________\n",
      "layer4_2 (Dense)             (3, 100)                  15100     \n",
      "_________________________________________________________________\n",
      "bp1_3 (BatchNormalization)   (3, 100)                  400       \n",
      "_________________________________________________________________\n",
      "layer1_3 (Dense)             (3, 150)                  15150     \n",
      "_________________________________________________________________\n",
      "bp2_3 (BatchNormalization)   (3, 150)                  600       \n",
      "_________________________________________________________________\n",
      "layer2_3 (Dense)             (3, 150)                  22650     \n",
      "_________________________________________________________________\n",
      "bp3_3 (BatchNormalization)   (3, 150)                  600       \n",
      "_________________________________________________________________\n",
      "layer3_3 (Dense)             (3, 150)                  22650     \n",
      "_________________________________________________________________\n",
      "pb4_3 (BatchNormalization)   (3, 150)                  600       \n",
      "_________________________________________________________________\n",
      "layer4_3 (Dense)             (3, 100)                  15100     \n",
      "_________________________________________________________________\n",
      "bp1_4 (BatchNormalization)   (3, 100)                  400       \n",
      "_________________________________________________________________\n",
      "layer1_4 (Dense)             (3, 150)                  15150     \n",
      "_________________________________________________________________\n",
      "bp2_4 (BatchNormalization)   (3, 150)                  600       \n",
      "_________________________________________________________________\n",
      "layer2_4 (Dense)             (3, 150)                  22650     \n",
      "_________________________________________________________________\n",
      "bp3_4 (BatchNormalization)   (3, 150)                  600       \n",
      "_________________________________________________________________\n",
      "layer3_4 (Dense)             (3, 150)                  22650     \n",
      "_________________________________________________________________\n",
      "pb4_4 (BatchNormalization)   (3, 150)                  600       \n",
      "_________________________________________________________________\n",
      "layer4_4 (Dense)             (3, 100)                  15100     \n",
      "_________________________________________________________________\n",
      "bp1_5 (BatchNormalization)   (3, 100)                  400       \n",
      "_________________________________________________________________\n",
      "layer1_5 (Dense)             (3, 150)                  15150     \n",
      "_________________________________________________________________\n",
      "bp2_5 (BatchNormalization)   (3, 150)                  600       \n",
      "_________________________________________________________________\n",
      "layer2_5 (Dense)             (3, 150)                  22650     \n",
      "_________________________________________________________________\n",
      "bp3_5 (BatchNormalization)   (3, 150)                  600       \n",
      "_________________________________________________________________\n",
      "layer3_5 (Dense)             (3, 150)                  22650     \n",
      "_________________________________________________________________\n",
      "pb4_5 (BatchNormalization)   (3, 150)                  600       \n",
      "_________________________________________________________________\n",
      "layer4_5 (Dense)             (3, 100)                  15100     \n",
      "_________________________________________________________________\n",
      "bp1_6 (BatchNormalization)   (3, 100)                  400       \n",
      "_________________________________________________________________\n",
      "layer1_6 (Dense)             (3, 150)                  15150     \n",
      "_________________________________________________________________\n",
      "bp2_6 (BatchNormalization)   (3, 150)                  600       \n",
      "_________________________________________________________________\n",
      "layer2_6 (Dense)             (3, 150)                  22650     \n",
      "_________________________________________________________________\n",
      "bp3_6 (BatchNormalization)   (3, 150)                  600       \n",
      "_________________________________________________________________\n",
      "layer3_6 (Dense)             (3, 150)                  22650     \n",
      "_________________________________________________________________\n",
      "pb4_6 (BatchNormalization)   (3, 150)                  600       \n",
      "_________________________________________________________________\n",
      "layer4_6 (Dense)             (3, 100)                  15100     \n",
      "=================================================================\n",
      "Total params: 458,800\n",
      "Trainable params: 452,300\n",
      "Non-trainable params: 6,500\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "decoder_norm_loss_3_g = keras.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.BatchNormalization(name=\"bp1\"),\n",
    "        layers.Dense(user_number + code_length, activation=\"relu\", name=\"layer1\"),\n",
    "        tf.keras.layers.BatchNormalization(name=\"bp2\"),\n",
    "        layers.Dense(user_number + code_length, activation=\"relu\", name=\"layer2\"),\n",
    "        tf.keras.layers.BatchNormalization(name=\"bp3\"),\n",
    "        layers.Dense(user_number + code_length, activation=\"relu\", name=\"layer3\"),\n",
    "        tf.keras.layers.BatchNormalization(name=\"pb4\"),\n",
    "        layers.Dense(user_number, activation=\"sigmoid\", name=\"layer4\"),\n",
    "        tf.keras.layers.BatchNormalization(name=\"bp1_2\"),\n",
    "        layers.Dense(user_number + code_length, activation=\"relu\", name=\"layer1_2\"),\n",
    "        tf.keras.layers.BatchNormalization(name=\"bp2_2\"),\n",
    "        layers.Dense(user_number + code_length, activation=\"relu\", name=\"layer2_2\"),\n",
    "        tf.keras.layers.BatchNormalization(name=\"bp3_2\"),\n",
    "        layers.Dense(user_number + code_length, activation=\"relu\", name=\"layer3_2\"),\n",
    "        tf.keras.layers.BatchNormalization(name=\"pb4_2\"),\n",
    "        layers.Dense(user_number, activation=\"relu\", name=\"layer4_2\"),\n",
    "        tf.keras.layers.BatchNormalization(name=\"bp1_3\"),\n",
    "        layers.Dense(user_number + code_length, activation=\"relu\", name=\"layer1_3\"),\n",
    "        tf.keras.layers.BatchNormalization(name=\"bp2_3\"),\n",
    "        layers.Dense(user_number + code_length, activation=\"relu\", name=\"layer2_3\"),\n",
    "        tf.keras.layers.BatchNormalization(name=\"bp3_3\"),\n",
    "        layers.Dense(user_number + code_length, activation=\"relu\", name=\"layer3_3\"),\n",
    "        tf.keras.layers.BatchNormalization(name=\"pb4_3\"),\n",
    "        layers.Dense(user_number, activation=\"sigmoid\", name=\"layer4_3\"),\n",
    "        tf.keras.layers.BatchNormalization(name=\"bp1_4\"),\n",
    "        layers.Dense(user_number + code_length, activation=\"relu\", name=\"layer1_4\"),\n",
    "        tf.keras.layers.BatchNormalization(name=\"bp2_4\"),\n",
    "        layers.Dense(user_number + code_length, activation=\"relu\", name=\"layer2_4\"),\n",
    "        tf.keras.layers.BatchNormalization(name=\"bp3_4\"),\n",
    "        layers.Dense(user_number + code_length, activation=\"relu\", name=\"layer3_4\"),\n",
    "        tf.keras.layers.BatchNormalization(name=\"pb4_4\"),\n",
    "        layers.Dense(user_number, activation=\"relu\", name=\"layer4_4\"),\n",
    "        tf.keras.layers.BatchNormalization(name=\"bp1_5\"),\n",
    "        layers.Dense(user_number + code_length, activation=\"relu\", name=\"layer1_5\"),\n",
    "        tf.keras.layers.BatchNormalization(name=\"bp2_5\"),\n",
    "        layers.Dense(user_number + code_length, activation=\"relu\", name=\"layer2_5\"),\n",
    "        tf.keras.layers.BatchNormalization(name=\"bp3_5\"),\n",
    "        layers.Dense(user_number + code_length, activation=\"relu\", name=\"layer3_5\"),\n",
    "        tf.keras.layers.BatchNormalization(name=\"pb4_5\"),\n",
    "        layers.Dense(user_number, activation=\"sigmoid\", name=\"layer4_5\"),\n",
    "        tf.keras.layers.BatchNormalization(name=\"bp1_6\"),\n",
    "        layers.Dense(user_number + code_length, activation=\"relu\", name=\"layer1_6\"),\n",
    "        tf.keras.layers.BatchNormalization(name=\"bp2_6\"),\n",
    "        layers.Dense(user_number + code_length, activation=\"relu\", name=\"layer2_6\"),\n",
    "        tf.keras.layers.BatchNormalization(name=\"bp3_6\"),\n",
    "        layers.Dense(user_number + code_length, activation=\"relu\", name=\"layer3_6\"),\n",
    "        tf.keras.layers.BatchNormalization(name=\"pb4_6\"),\n",
    "        layers.Dense(user_number, activation=\"relu\", name=\"layer4_6\")\n",
    "    ],name = 'decoder'\n",
    ")\n",
    "# Call model on a test input\n",
    "y = tf.ones((3, code_length))\n",
    "x_hat = decoder_norm_loss_3_g(y)\n",
    "decoder_norm_loss_3_g.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "2000/2000 [==============================] - 167s 80ms/step - loss: 0.1739 - nmse_accuracy: -0.6752 - val_loss: 0.1442 - val_nmse_accuracy: -1.5211\n",
      "Epoch 2/30\n",
      "2000/2000 [==============================] - 153s 77ms/step - loss: 0.1319 - nmse_accuracy: -1.9533 - val_loss: 0.1262 - val_nmse_accuracy: -2.1576\n",
      "Epoch 3/30\n",
      "2000/2000 [==============================] - 161s 81ms/step - loss: 0.1207 - nmse_accuracy: -2.3606 - val_loss: 0.1189 - val_nmse_accuracy: -2.4308\n",
      "Epoch 4/30\n",
      "2000/2000 [==============================] - 157s 79ms/step - loss: 0.1148 - nmse_accuracy: -2.5665 - val_loss: 0.1137 - val_nmse_accuracy: -2.6470\n",
      "Epoch 5/30\n",
      "2000/2000 [==============================] - 186s 93ms/step - loss: 0.1097 - nmse_accuracy: -2.7604 - val_loss: 0.8429 - val_nmse_accuracy: 8.5883\n",
      "Epoch 6/30\n",
      "2000/2000 [==============================] - 160s 80ms/step - loss: 0.1053 - nmse_accuracy: -2.9837 - val_loss: 0.1030 - val_nmse_accuracy: -3.1160\n",
      "Epoch 7/30\n",
      "2000/2000 [==============================] - 167s 84ms/step - loss: 0.1014 - nmse_accuracy: -3.1468 - val_loss: 0.0998 - val_nmse_accuracy: -3.2399\n",
      "Epoch 8/30\n",
      "2000/2000 [==============================] - 163s 82ms/step - loss: 0.0976 - nmse_accuracy: -3.3480 - val_loss: 0.0980 - val_nmse_accuracy: -3.3392\n",
      "Epoch 9/30\n",
      "2000/2000 [==============================] - 148s 74ms/step - loss: 0.0939 - nmse_accuracy: -3.4785 - val_loss: 0.0968 - val_nmse_accuracy: -3.3877\n",
      "Epoch 10/30\n",
      "2000/2000 [==============================] - 162s 81ms/step - loss: 0.0913 - nmse_accuracy: -3.6360 - val_loss: 0.0917 - val_nmse_accuracy: -3.6352\n",
      "Epoch 11/30\n",
      "2000/2000 [==============================] - 155s 78ms/step - loss: 0.0877 - nmse_accuracy: -3.8496 - val_loss: 0.0911 - val_nmse_accuracy: -3.6807\n",
      "Epoch 12/30\n",
      "2000/2000 [==============================] - 168s 84ms/step - loss: 0.0859 - nmse_accuracy: -3.9580 - val_loss: 0.0929 - val_nmse_accuracy: -3.5968\n",
      "Epoch 13/30\n",
      "2000/2000 [==============================] - 144s 72ms/step - loss: 0.0845 - nmse_accuracy: -4.0234 - val_loss: 0.0932 - val_nmse_accuracy: -3.5518\n",
      "Epoch 14/30\n",
      "2000/2000 [==============================] - 168s 84ms/step - loss: 0.0834 - nmse_accuracy: -4.0683 - val_loss: 0.0861 - val_nmse_accuracy: -3.9550\n",
      "Epoch 15/30\n",
      "2000/2000 [==============================] - 184s 92ms/step - loss: 0.0811 - nmse_accuracy: -4.2119 - val_loss: 1.3398 - val_nmse_accuracy: 7.1228\n",
      "Epoch 16/30\n",
      "2000/2000 [==============================] - 144s 72ms/step - loss: 0.0793 - nmse_accuracy: -4.3066 - val_loss: 0.8007 - val_nmse_accuracy: 5.2443\n",
      "Epoch 17/30\n",
      "2000/2000 [==============================] - 138s 69ms/step - loss: 0.0761 - nmse_accuracy: -4.5195 - val_loss: 0.1434 - val_nmse_accuracy: -2.5376\n",
      "Epoch 18/30\n",
      "2000/2000 [==============================] - 138s 69ms/step - loss: 0.0734 - nmse_accuracy: -4.7045 - val_loss: 0.1413 - val_nmse_accuracy: -2.2443\n",
      "Epoch 19/30\n",
      "2000/2000 [==============================] - 138s 69ms/step - loss: 0.0720 - nmse_accuracy: -4.7660 - val_loss: 0.0806 - val_nmse_accuracy: -4.2700\n",
      "Epoch 20/30\n",
      "2000/2000 [==============================] - 138s 69ms/step - loss: 0.0724 - nmse_accuracy: -4.7586 - val_loss: 1.1875 - val_nmse_accuracy: 7.4137\n",
      "Epoch 21/30\n",
      "2000/2000 [==============================] - 139s 70ms/step - loss: 0.0724 - nmse_accuracy: -4.7763 - val_loss: 0.0871 - val_nmse_accuracy: -4.1011\n",
      "Epoch 22/30\n",
      "2000/2000 [==============================] - 138s 69ms/step - loss: 0.0717 - nmse_accuracy: -4.8234 - val_loss: 0.0747 - val_nmse_accuracy: -4.6548\n",
      "Epoch 23/30\n",
      "2000/2000 [==============================] - 139s 69ms/step - loss: 0.0719 - nmse_accuracy: -4.7926 - val_loss: 0.0761 - val_nmse_accuracy: -4.5349\n",
      "Epoch 24/30\n",
      "2000/2000 [==============================] - 139s 69ms/step - loss: 0.0717 - nmse_accuracy: -4.8099 - val_loss: 6.5677 - val_nmse_accuracy: 14.1227\n",
      "Epoch 25/30\n",
      "2000/2000 [==============================] - 137s 69ms/step - loss: 0.0730 - nmse_accuracy: -4.7515 - val_loss: 10.9229 - val_nmse_accuracy: 16.2135\n",
      "Epoch 26/30\n",
      "2000/2000 [==============================] - 138s 69ms/step - loss: 0.0732 - nmse_accuracy: -4.7027 - val_loss: 3.1268 - val_nmse_accuracy: 11.9767\n",
      "Epoch 27/30\n",
      "2000/2000 [==============================] - 139s 69ms/step - loss: 0.0727 - nmse_accuracy: -4.7503 - val_loss: 19.7505 - val_nmse_accuracy: 19.1644\n",
      "Epoch 28/30\n",
      "2000/2000 [==============================] - 141s 70ms/step - loss: 0.0736 - nmse_accuracy: -4.6850 - val_loss: 106.7816 - val_nmse_accuracy: 25.9227\n",
      "Epoch 29/30\n",
      "2000/2000 [==============================] - 139s 70ms/step - loss: 0.0750 - nmse_accuracy: -4.5966 - val_loss: 2.6384 - val_nmse_accuracy: 8.9817\n",
      "Epoch 30/30\n",
      "2000/2000 [==============================] - 139s 70ms/step - loss: 0.0751 - nmse_accuracy: -4.5796 - val_loss: 17.4822 - val_nmse_accuracy: 18.0009\n",
      "313/313 - 1s - loss: 0.0801 - nmse_accuracy: -4.1628e+00\n",
      "Test loss: 0.08011970669031143\n",
      "Test accuracy: -4.162769317626953\n"
     ]
    }
   ],
   "source": [
    "decoder_norm_loss_3_g.compile(\n",
    "    loss=keras.losses.MeanSquaredError(),\n",
    "    optimizer=keras.optimizers.RMSprop(),\n",
    "    metrics=NMSE_Accuracy(),\n",
    ")\n",
    "\n",
    "history = decoder_norm_loss_3_g.fit(y_set, h_set, batch_size=1000, epochs=30, validation_split=0.2)\n",
    "\n",
    "test_scores = decoder_norm_loss_3_g.evaluate(y_test, h_test, verbose=2)\n",
    "print(\"Test loss:\", test_scores[0])\n",
    "\n",
    "print(\"Test accuracy:\", test_scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for pretreatment\n",
    "original_inputs = tf.keras.Input(shape=(code_length,), name=\"original_inputs\")\n",
    "bn_input = keras.layers.BatchNormalization()(original_inputs)\n",
    "layer_1 = layers.Dense(user_number + code_length, activation=\"sigmoid\")(bn_input)\n",
    "pretreatment = tf.keras.Model(inputs=original_inputs, outputs=layer_1, name=\"pretreatment\")\n",
    "\n",
    "# estimator\n",
    "estimator_inputs = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "938/938 [==============================] - 3s 2ms/step - loss: 0.0952\n",
      "Epoch 2/3\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.0677A:\n",
      "Epoch 3/3\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.0675\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x14873e040>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Sampling(layers.Layer):\n",
    "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "original_dim = 784\n",
    "intermediate_dim = 64\n",
    "latent_dim = 32\n",
    "\n",
    "# Define encoder model.\n",
    "original_inputs = tf.keras.Input(shape=(original_dim,), name=\"encoder_input\")\n",
    "x = layers.Dense(intermediate_dim, activation=\"relu\")(original_inputs)\n",
    "z_mean = layers.Dense(latent_dim, name=\"z_mean\")(x)\n",
    "z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(x)\n",
    "z = Sampling()((z_mean, z_log_var))\n",
    "encoder = tf.keras.Model(inputs=original_inputs, outputs=z, name=\"encoder\")\n",
    "\n",
    "# Define decoder model.\n",
    "latent_inputs = tf.keras.Input(shape=(latent_dim,), name=\"z_sampling\")\n",
    "x = layers.Dense(intermediate_dim, activation=\"relu\")(latent_inputs)\n",
    "outputs = layers.Dense(original_dim, activation=\"sigmoid\")(x)\n",
    "decoder = tf.keras.Model(inputs=latent_inputs, outputs=outputs, name=\"decoder\")\n",
    "\n",
    "# Define VAE model.\n",
    "outputs = decoder(z)\n",
    "vae = tf.keras.Model(inputs=original_inputs, outputs=outputs, name=\"vae\")\n",
    "\n",
    "# Add KL divergence regularization loss.\n",
    "kl_loss = -0.5 * tf.reduce_mean(z_log_var - tf.square(z_mean) - tf.exp(z_log_var) + 1)\n",
    "vae.add_loss(kl_loss)\n",
    "\n",
    "# Train.\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "vae.compile(optimizer, loss=tf.keras.losses.MeanSquaredError())\n",
    "vae.fit(x_train, x_train, epochs=3, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, _), _ = tf.keras.datasets.mnist.load_data()\n",
    "x_train = x_train.reshape(60000, 784).astype(\"float32\") / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 784), dtype=float32, numpy=\n",
       "array([[0.00481367, 0.0061579 , 0.00658953, 0.00625724, 0.00555784,\n",
       "        0.00955978, 0.00852484, 0.00730067, 0.00897959, 0.01063415,\n",
       "        0.00852767, 0.00623   , 0.01061061, 0.00745192, 0.00635117,\n",
       "        0.00514427, 0.00715104, 0.00805426, 0.00541797, 0.00664061,\n",
       "        0.00687668, 0.00676125, 0.00766245, 0.0107075 , 0.00626695,\n",
       "        0.00671542, 0.00782353, 0.00639156, 0.00724027, 0.00789261,\n",
       "        0.00655583, 0.00578952, 0.00685656, 0.00500193, 0.00696513,\n",
       "        0.01023212, 0.0072414 , 0.00832132, 0.00858665, 0.00518015,\n",
       "        0.00726119, 0.01309493, 0.00501278, 0.00542459, 0.00546834,\n",
       "        0.01120231, 0.00845298, 0.00579092, 0.00828516, 0.00533614,\n",
       "        0.00630704, 0.00962543, 0.00952736, 0.00569856, 0.00840023,\n",
       "        0.00900868, 0.01027164, 0.00621584, 0.00545567, 0.00659001,\n",
       "        0.0065653 , 0.00514942, 0.00744674, 0.00969073, 0.00749481,\n",
       "        0.00909033, 0.009561  , 0.00830233, 0.01517773, 0.01212364,\n",
       "        0.01443157, 0.01900154, 0.015046  , 0.02001399, 0.01479682,\n",
       "        0.01225281, 0.00989041, 0.00630951, 0.00714657, 0.00762519,\n",
       "        0.00663823, 0.00690442, 0.00513998, 0.00925568, 0.00944957,\n",
       "        0.00884801, 0.00910205, 0.00844774, 0.00697029, 0.0088191 ,\n",
       "        0.00667971, 0.00676712, 0.01075324, 0.01178122, 0.01905417,\n",
       "        0.02469373, 0.0403896 , 0.04595864, 0.06242913, 0.05983916,\n",
       "        0.06739551, 0.05442363, 0.04559478, 0.03058374, 0.01639536,\n",
       "        0.01195112, 0.00971627, 0.00709048, 0.0066112 , 0.00698805,\n",
       "        0.00561354, 0.00481793, 0.00672063, 0.00680318, 0.00788835,\n",
       "        0.00625926, 0.00575745, 0.00851116, 0.00987232, 0.01166019,\n",
       "        0.01785544, 0.0362623 , 0.05547172, 0.09020928, 0.11090589,\n",
       "        0.16342899, 0.18670821, 0.19579679, 0.20096165, 0.1939691 ,\n",
       "        0.14894855, 0.11267442, 0.07488397, 0.03905001, 0.02514148,\n",
       "        0.01400509, 0.00827798, 0.00369719, 0.00599408, 0.00542691,\n",
       "        0.00531253, 0.00645542, 0.00785485, 0.00924543, 0.00965163,\n",
       "        0.00777382, 0.01312891, 0.03836527, 0.05591062, 0.09077942,\n",
       "        0.14305392, 0.19195169, 0.23979598, 0.29075998, 0.34603363,\n",
       "        0.36438915, 0.37485936, 0.34521002, 0.28690338, 0.22470906,\n",
       "        0.16273984, 0.11308241, 0.06402579, 0.03563434, 0.01144284,\n",
       "        0.00679332, 0.00750607, 0.0067274 , 0.00462109, 0.00787345,\n",
       "        0.0046328 , 0.01197168, 0.00890771, 0.01420933, 0.03785998,\n",
       "        0.06718403, 0.11042109, 0.17225102, 0.2389552 , 0.30547923,\n",
       "        0.36849767, 0.42298704, 0.47131407, 0.48759073, 0.49164638,\n",
       "        0.45113975, 0.38320366, 0.31003416, 0.24254224, 0.16039261,\n",
       "        0.10869721, 0.05966833, 0.03158671, 0.00998977, 0.00609913,\n",
       "        0.00865975, 0.00498745, 0.0081614 , 0.00945869, 0.00833303,\n",
       "        0.01526484, 0.03847113, 0.05702028, 0.10619444, 0.17370126,\n",
       "        0.23691696, 0.33051598, 0.38672823, 0.45340762, 0.47924033,\n",
       "        0.48825893, 0.5225718 , 0.5190275 , 0.5133811 , 0.44777492,\n",
       "        0.34832105, 0.29218665, 0.21321213, 0.13564977, 0.07548881,\n",
       "        0.03610608, 0.01139364, 0.00953743, 0.00737867, 0.00937456,\n",
       "        0.00627142, 0.00867635, 0.0080139 , 0.01745665, 0.04447463,\n",
       "        0.07381567, 0.13347685, 0.21834421, 0.28160554, 0.37262166,\n",
       "        0.43883008, 0.45489928, 0.4497013 , 0.4524992 , 0.47945043,\n",
       "        0.4945296 , 0.5210025 , 0.47967717, 0.3864241 , 0.32414314,\n",
       "        0.23915118, 0.1476357 , 0.08583567, 0.03093264, 0.01544917,\n",
       "        0.00682476, 0.00779775, 0.00676358, 0.00980327, 0.00501078,\n",
       "        0.00928199, 0.02375132, 0.04969549, 0.08095986, 0.1481969 ,\n",
       "        0.22554487, 0.330094  , 0.4153068 , 0.44446707, 0.42534983,\n",
       "        0.41310036, 0.40230983, 0.40504396, 0.42259803, 0.47489354,\n",
       "        0.45984957, 0.37610328, 0.32685742, 0.25077564, 0.15332139,\n",
       "        0.07329568, 0.02795625, 0.00872645, 0.0076302 , 0.00628239,\n",
       "        0.00618109, 0.00929236, 0.00940496, 0.01268923, 0.0198085 ,\n",
       "        0.03567564, 0.07970002, 0.1479275 , 0.2502214 , 0.36803436,\n",
       "        0.442755  , 0.4284522 , 0.38689637, 0.34860674, 0.34022522,\n",
       "        0.35643202, 0.36818027, 0.43469685, 0.42045897, 0.36382714,\n",
       "        0.3164487 , 0.23599336, 0.1430288 , 0.06108958, 0.02209178,\n",
       "        0.01153994, 0.00814313, 0.00929776, 0.00692102, 0.00766507,\n",
       "        0.01310441, 0.01525098, 0.0214029 , 0.04662803, 0.08738583,\n",
       "        0.15638554, 0.2574271 , 0.38688555, 0.4350075 , 0.39608425,\n",
       "        0.34875888, 0.3140862 , 0.31090933, 0.33633772, 0.39352158,\n",
       "        0.44675413, 0.42061633, 0.35347998, 0.29012668, 0.20302132,\n",
       "        0.12622273, 0.06776518, 0.01685163, 0.00989568, 0.00602624,\n",
       "        0.00655067, 0.01073092, 0.00655037, 0.00928417, 0.00861517,\n",
       "        0.01405349, 0.04692101, 0.09782097, 0.17525336, 0.28803858,\n",
       "        0.39052373, 0.41846937, 0.3738125 , 0.34734547, 0.33323094,\n",
       "        0.35810435, 0.4154119 , 0.4768942 , 0.47058898, 0.44603494,\n",
       "        0.35653552, 0.2643885 , 0.16939574, 0.11224812, 0.05944529,\n",
       "        0.02810255, 0.00999269, 0.00618011, 0.0051381 , 0.00598535,\n",
       "        0.00826946, 0.00581515, 0.00468504, 0.00902739, 0.0385772 ,\n",
       "        0.09534496, 0.18972164, 0.30866525, 0.40309623, 0.41737622,\n",
       "        0.37903526, 0.36565006, 0.39342317, 0.456633  , 0.517915  ,\n",
       "        0.5575812 , 0.514516  , 0.4546938 , 0.36119443, 0.24775109,\n",
       "        0.1757021 , 0.09218556, 0.06182447, 0.02511978, 0.00652334,\n",
       "        0.0057812 , 0.00536293, 0.00787067, 0.00757307, 0.00558102,\n",
       "        0.01328844, 0.01379135, 0.06149215, 0.10534945, 0.19369388,\n",
       "        0.31272954, 0.38252968, 0.39588305, 0.37593806, 0.4007643 ,\n",
       "        0.46897465, 0.5293542 , 0.56627184, 0.5671584 , 0.51562047,\n",
       "        0.45851105, 0.3532672 , 0.2538673 , 0.16823187, 0.1096471 ,\n",
       "        0.06022722, 0.02737829, 0.01236308, 0.00807899, 0.0075129 ,\n",
       "        0.00769871, 0.00608453, 0.01101473, 0.00583926, 0.02553689,\n",
       "        0.067949  , 0.1185711 , 0.19809747, 0.29226622, 0.36182007,\n",
       "        0.39102042, 0.39582682, 0.43548328, 0.49594286, 0.53506297,\n",
       "        0.5361543 , 0.50240594, 0.4744149 , 0.42900497, 0.3471514 ,\n",
       "        0.2640881 , 0.17818037, 0.12788206, 0.06087339, 0.02207255,\n",
       "        0.00652713, 0.01125962, 0.00707752, 0.00679231, 0.00664467,\n",
       "        0.00583979, 0.01009992, 0.02128321, 0.06910789, 0.14385647,\n",
       "        0.21947482, 0.27593327, 0.32696575, 0.36089218, 0.3870891 ,\n",
       "        0.43400478, 0.46557865, 0.47729447, 0.48749146, 0.4576949 ,\n",
       "        0.43075418, 0.3811641 , 0.3250771 , 0.2657349 , 0.18501031,\n",
       "        0.1230889 , 0.05896375, 0.01896414, 0.00725633, 0.00773275,\n",
       "        0.00652003, 0.00745568, 0.00618333, 0.00785631, 0.00809518,\n",
       "        0.0271925 , 0.07928163, 0.16282794, 0.21038619, 0.2532671 ,\n",
       "        0.27928466, 0.3188834 , 0.3572345 , 0.38364974, 0.40397978,\n",
       "        0.43141383, 0.4449689 , 0.43236685, 0.41631818, 0.3702286 ,\n",
       "        0.30872554, 0.26753145, 0.18982765, 0.11545131, 0.04421845,\n",
       "        0.02709797, 0.01152998, 0.00700322, 0.01020256, 0.00524908,\n",
       "        0.01020563, 0.00827736, 0.00893402, 0.03072721, 0.09243077,\n",
       "        0.15244606, 0.2154468 , 0.24381348, 0.28566462, 0.32600054,\n",
       "        0.3414498 , 0.3474915 , 0.3694092 , 0.40729192, 0.44895658,\n",
       "        0.4450881 , 0.41870108, 0.36737415, 0.30361542, 0.23389736,\n",
       "        0.16721052, 0.10283044, 0.05142322, 0.02430496, 0.00994232,\n",
       "        0.00955054, 0.00750557, 0.01015908, 0.00915834, 0.00835985,\n",
       "        0.01841128, 0.0474456 , 0.10163978, 0.1890563 , 0.22894126,\n",
       "        0.2797463 , 0.32114667, 0.34253183, 0.35081294, 0.3576684 ,\n",
       "        0.397103  , 0.4404179 , 0.4603035 , 0.4702578 , 0.41802844,\n",
       "        0.35195   , 0.27058125, 0.21143162, 0.1503771 , 0.07966295,\n",
       "        0.04458153, 0.0225451 , 0.00761136, 0.0081968 , 0.00847962,\n",
       "        0.00933391, 0.00486448, 0.01035741, 0.01138952, 0.03644535,\n",
       "        0.09510767, 0.1762824 , 0.25026622, 0.30900925, 0.349247  ,\n",
       "        0.38716102, 0.39306864, 0.41518983, 0.44939828, 0.4795167 ,\n",
       "        0.49379683, 0.47868788, 0.3974924 , 0.31732547, 0.22880086,\n",
       "        0.16444689, 0.1073319 , 0.06316915, 0.03408727, 0.01281881,\n",
       "        0.00627831, 0.00652933, 0.00611183, 0.00725073, 0.00615147,\n",
       "        0.00433156, 0.00951189, 0.03485209, 0.08302122, 0.17915758,\n",
       "        0.2483981 , 0.3106243 , 0.3777462 , 0.42660892, 0.43278682,\n",
       "        0.4544101 , 0.4830618 , 0.50042015, 0.4957289 , 0.4422478 ,\n",
       "        0.34761995, 0.2494387 , 0.17085668, 0.13459873, 0.08597019,\n",
       "        0.04053319, 0.01680446, 0.00956577, 0.00862405, 0.0079523 ,\n",
       "        0.00564548, 0.00863492, 0.00992706, 0.012043  , 0.00958672,\n",
       "        0.01888254, 0.04878497, 0.1282782 , 0.20962894, 0.2766413 ,\n",
       "        0.37548476, 0.42091516, 0.45220754, 0.48107192, 0.49513847,\n",
       "        0.4851823 , 0.44318533, 0.37436754, 0.28402895, 0.18152902,\n",
       "        0.12400067, 0.08618519, 0.05029249, 0.02732748, 0.00761726,\n",
       "        0.0117929 , 0.00857422, 0.00533575, 0.00795579, 0.00865385,\n",
       "        0.01107323, 0.01453778, 0.00567159, 0.01262251, 0.02960947,\n",
       "        0.06903699, 0.1226615 , 0.19777995, 0.26559505, 0.34812385,\n",
       "        0.38264865, 0.40677273, 0.4005992 , 0.3718375 , 0.32172394,\n",
       "        0.26224667, 0.18608406, 0.09930924, 0.06664434, 0.04008409,\n",
       "        0.01477641, 0.0147815 , 0.00753999, 0.00945473, 0.00695947,\n",
       "        0.00565696, 0.00575927, 0.00678518, 0.00937343, 0.0073072 ,\n",
       "        0.0077993 , 0.00649399, 0.01577318, 0.02171379, 0.04070136,\n",
       "        0.08283684, 0.11363927, 0.17161801, 0.20435151, 0.22431245,\n",
       "        0.23114488, 0.22388116, 0.18575868, 0.1304388 , 0.08308899,\n",
       "        0.04811615, 0.02833843, 0.01393384, 0.00877267, 0.00956622,\n",
       "        0.00627667, 0.00743437, 0.00706375, 0.00759852, 0.00692281,\n",
       "        0.00768408, 0.0069662 , 0.00683847, 0.0064055 , 0.00562996,\n",
       "        0.0080452 , 0.00815719, 0.01766157, 0.01884118, 0.05016029,\n",
       "        0.06938079, 0.09360966, 0.09115085, 0.09157896, 0.08954701,\n",
       "        0.06895658, 0.04532555, 0.04057196, 0.03207028, 0.01997524,\n",
       "        0.00723201, 0.00873491, 0.00773519, 0.00800481, 0.00685114,\n",
       "        0.00737223, 0.00499249, 0.00787941, 0.00783083, 0.00738892,\n",
       "        0.00567138, 0.00887865, 0.01113886, 0.00836256, 0.00729147,\n",
       "        0.01028883, 0.01269716, 0.01715729, 0.02661717, 0.0253835 ,\n",
       "        0.02812862, 0.03125015, 0.03405616, 0.0260576 , 0.0149017 ,\n",
       "        0.01341805, 0.01576579, 0.00824884, 0.00787437, 0.0087325 ,\n",
       "        0.00770086, 0.0071916 , 0.00666839, 0.00731435, 0.00742653,\n",
       "        0.00702694, 0.00634286, 0.00681236, 0.0068776 , 0.00856766,\n",
       "        0.00674129, 0.00507692, 0.00758719, 0.00914967, 0.00745416,\n",
       "        0.01066121, 0.01065761, 0.00721338, 0.00778657, 0.00734663,\n",
       "        0.01039314, 0.00459117, 0.00907752, 0.00484997, 0.00772914,\n",
       "        0.0077025 , 0.00683942, 0.01025769, 0.00744769, 0.00804946,\n",
       "        0.00616869, 0.00664979, 0.01058918, 0.00783601]], dtype=float32)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae(x_train[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8",
   "language": "python",
   "name": "python3.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
